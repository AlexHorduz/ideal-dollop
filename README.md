# Опис
Гітхаб https://github.com/AlexHorduz/ideal-dollop
Датасет https://www.kaggle.com/datasets/pkdarabi/cardetection/data

# Частина 1

## EDA
Датасет складається з 5к зображень. Деякі фото зроблені на FishEye камеру, що додає різноманітності в датасет.
Всього 15 класів. Більше всього прикладів мають класи "Red Light" та "Green Light". Клас "Speed Limit 10" майже не має прикладів. Всі інші класи розподілені +- рівномірно.
80% зображень мають лише по одному об'єкту, 10% - 2.

## Data Preprocessing
Letterbox-ресайз з паддінгом: зберігає співвідношення сторін без деформації об’єктів
і вписує зображення у фіксований розмір; коректно масштабує та зсуває бокси

Випадкове горизонтальне віддзеркалення підвищує інваріантність до напрямку,
покращує узагальнення для детекції. Для дорожніх об’єктів це безпечна і
ефективна аугментація, але варто бути обережним з семантично-асиметричними знаками.

Color Jitter моделює різні умови освітлення та камер

ToTensor + Normalize нормалізація прискорює та стабілізує навчання,
значення mean/std відповідають загальноприйнятим практикам для моделей torchvision

# Частина 2

## Metrics

Ми будемо використовувати наступні метрики для оцінки якості моделей:
- mAP (mean average prevision). Середня точність, усереднена по всім класам. Дана метрика використовує AP (average precision), який обраховується для конкретного класу за заданим трешхолдом IoU (intersection over union). Ми будемо використовувати AP@[0.5:0.95], тобто AP, усереднене для значень трешхолду від 0.5 до 0.95 з кроком 0.05. Дана метрика дає загальне розуміння якості моделі, проте не дає інформації про ефективність на конкретному класі. 
- Precision, recall по кожному класу для IoU=0.5. Спочатку обраховуємо classification confidence threshold, оптимізуючи f1-score. Потім, маючи даний трешхолд, рахуємо Precision та Recall для кожного класу. Дана гранульована метрика дозволить проаналізувати ефективність моделі на кожному з класів.

## Losses

Для навчання моделі використовуються наступні компоненти функції втрат:
- **Box loss (L_box)**. Визначає точність локалізації об’єкта. Обраховується як `1 - IoU(pred, gt)` між запредікченим та справжнім bounding box. Таким чином, чим краще перекриття між прогнозованим баундін боксом та ground truth, тим менше значення функції. Даний компонент дозволяє моделі навчитися точно позиціонувати об’єкти в межах зображення.  
- **Objectness loss (L_obj)**. Визначає, чи містить конкретна сіткова комірка (cell) об’єкт. Для кожного передбаченого баундін боксу модель прогнозує значення **objectness score**. Якщо рамка відповідає реальному об’єкту — таргет дорівнює `1`, інакше — `0`. Обчислюється за допомогою **Binary Cross Entropy (BCE)**: `L_obj = BCE(p_obj, y_obj)`. Даний компонент допомагає моделі ефективно відрізняти області, де є об’єкти, від бекграунду, що особливо важливо при невеликій кількості об’єктів на зображенні.  
- **Classification loss (L_cls)**. Визначає, наскільки правильно модель класифікує знайдений об’єкт. Оскільки у нас 15 класів, використовується **Cross Entropy (CE)** між передбаченим розподілом і справжньою міткою класу: `L_cls = CE(p_class, y_class)`. Цей компонент дозволяє моделі розрізняти об’єкти різних типів.  

Підсумкова функція втрат є зваженою сумою трьох компонентів:

`L_total = λ_box * L_box + λ_obj * L_obj + λ_cls * L_cls`

де коефіцієнти `λ_box`, `λ_obj`, `λ_cls` визначають відносну вагу кожної частини втрати. Типові значення:
- `λ_box = 0.05`
- `λ_obj = 1.0`
- `λ_cls = 0.5`
